{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4",
      "mount_file_id": "1lLwPHhjzyvScUhCjylsHP2PXchO8rz1J",
      "authorship_tag": "ABX9TyN7Fv+hLn2reJ6pixMTp6xS",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Sar-thak-3/Extract/blob/main/TextToQuestions.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import json\n",
        "f1 = open(\"/content/drive/MyDrive/TextQuestions/train_v0.2.json\")\n",
        "data1 = json.load(f1)\n",
        "f2 = open(\"/content/drive/MyDrive/TextQuestions/train-v1.1.json\")\n",
        "data2 = json.load(f2)"
      ],
      "metadata": {
        "id": "vjzJqctScBX2"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# questions = []\n",
        "# paragraphs = []\n",
        "\n",
        "# for item in data1[\"data\"][:]:\n",
        "#   paraques = []\n",
        "#   para = []\n",
        "#   for context in item[\"paragraphs\"]:\n",
        "#     para.append(context[\"context\"])\n",
        "#     contextques = []\n",
        "#     for qas in context[\"qas\"]:\n",
        "#       contextques.append(qas[\"question\"])\n",
        "\n",
        "#     paraques.append(contextques)\n",
        "\n",
        "#   questions.append(paraques)\n",
        "#   paragraphs.append(para)\n",
        "\n",
        "# for item in data2[\"data\"][:]:\n",
        "#   paraques = []\n",
        "#   para = []\n",
        "#   for context in item[\"paragraphs\"]:\n",
        "#     para.append(context[\"context\"])\n",
        "#     contextques = []\n",
        "#     for qas in context[\"qas\"]:\n",
        "#       contextques.append(qas[\"question\"])\n",
        "\n",
        "#     paraques.append(contextques)\n",
        "\n",
        "#   questions.append(paraques)\n",
        "#   paragraphs.append(para)"
      ],
      "metadata": {
        "id": "hVszLm_9dmTs"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "questions = []\n",
        "paragraphs = []\n",
        "\n",
        "for item in data1[\"data\"][:]:\n",
        "  for context in item[\"paragraphs\"]:\n",
        "    para = context[\"context\"]\n",
        "    ques = \"\"\n",
        "    for qas in context[\"qas\"]:\n",
        "      ques += qas[\"question\"]\n",
        "\n",
        "    questions.append(ques)\n",
        "    paragraphs.append(para)\n",
        "\n",
        "\n",
        "for item in data2[\"data\"][:]:\n",
        "  for context in item[\"paragraphs\"]:\n",
        "    para = context[\"context\"]\n",
        "    ques = \"\"\n",
        "    for qas in context[\"qas\"]:\n",
        "      ques += qas[\"question\"]\n",
        "      ques += \" \"\n",
        "      # questions.append(qas[\"question\"])\n",
        "\n",
        "    questions.append(ques)\n",
        "    paragraphs.append(para)"
      ],
      "metadata": {
        "id": "KmPbTQi_kDlm"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "questions[1]"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 70
        },
        "id": "XucG5nJsh9ln",
        "outputId": "d922c27d-d866-4712-b71f-917521a17f8f"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'what language do they speak?Do they speak any other languages?any literary items of interest?How old is their literature?were any of the poets listed by name?anything else of interest?any more recent literary works from them?'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 3
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "len(questions)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "v9y2XBOlhGDo",
        "outputId": "e1c1c0bb-451c-4bd1-a74f-573a84f562f8"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "30463"
            ]
          },
          "metadata": {},
          "execution_count": 4
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "len(paragraphs)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "oQihg4mWkjTN",
        "outputId": "d3ca5cea-2be3-48af-a44d-74cf5f444b48"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "30463"
            ]
          },
          "metadata": {},
          "execution_count": 5
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Regexp of text and questions"
      ],
      "metadata": {
        "id": "1GwlYy4rqwqN"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import re\n",
        "def clean_text(sentence):\n",
        "  sentence = sentence.lower()\n",
        "  sentence = re.sub(\"[^a-z]+\",\" \",sentence)\n",
        "  sentence = sentence.split()\n",
        "  sentence = \" \".join(sentence)\n",
        "  return sentence"
      ],
      "metadata": {
        "id": "fH6mlYXkq2IR"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "clean_text(questions[0])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 70
        },
        "id": "kOX-AS5wryVC",
        "outputId": "a22b0617-2998-4c51-a22d-020a5fa6d348"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'where is malayali located what other languages are spoken there what else is this place known for were they ever successful in doing this do they produce anything from here is this population still growing is the country thriving'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 7
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "for index,ques in enumerate(questions):\n",
        "  questions[index] = clean_text(ques)"
      ],
      "metadata": {
        "id": "GoBuhzvRrhsi"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "for index,para in enumerate(paragraphs):\n",
        "  paragraphs[index] = clean_text(para)"
      ],
      "metadata": {
        "id": "mQXnBhClsTBZ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "\n",
        "f = open(\"/content/drive/MyDrive/Image_captioning/glove.6B.50d.txt\" , encoding='utf8')\n",
        "embedding_index = {}\n",
        "embedding_to_word = {}\n",
        "\n",
        "for line in f:\n",
        "  values = line.split()\n",
        "  word = values[0]\n",
        "  word_embedding = np.array(values[1:],dtype='float')\n",
        "  embedding_to_word[word_embedding] = word\n",
        "  embedding_index[word] = word_embedding"
      ],
      "metadata": {
        "id": "oNv8-7eItuD-"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "C19iGJ8xAK8N"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import nltk\n",
        "from nltk.corpus import stopwords\n",
        "nltk.download('stopwords')\n",
        "stopwords = stopwords.words(\"english\")\n",
        "from nltk.stem import WordNetLemmatizer\n",
        "nltk.download('wordnet')\n",
        "wordnet_lemmatizer = WordNetLemmatizer()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "sHoMWpGSE-nD",
        "outputId": "e287d564-9e8c-462e-ad6f-85d1c93b5c4a"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Unzipping corpora/stopwords.zip.\n",
            "[nltk_data] Downloading package wordnet to /root/nltk_data...\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "clean_questions = []\n",
        "clean_paragraphs = []\n",
        "\n",
        "for ques in questions:\n",
        "  qs = []\n",
        "  [qs.append(wordnet_lemmatizer.lemmatize(word)) for word in ques.split() if word in embedding_index.keys() and word not in stopwords]\n",
        "  clean_questions.append(\" \".join(qs))"
      ],
      "metadata": {
        "id": "5sI7LlKzuZzW"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "for para in paragraphs:\n",
        "  ps = []\n",
        "  [ps.append(wordnet_lemmatizer.lemmatize(word)) for word in para.split() if word in embedding_index.keys() and word not in stopwords]\n",
        "  clean_paragraphs.append(\" \".join(ps))"
      ],
      "metadata": {
        "id": "mPDQI3c5u40A"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "total_words = []\n",
        "for ques in clean_questions:\n",
        "    [total_words.append(word) for word in ques.split()]\n",
        "print(len(total_words))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "RL1o4-hWVj-V",
        "outputId": "21b601c8-ea56-4d1f-a2ef-0d8deebf080e"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "667975\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "for para in clean_paragraphs:\n",
        "  [total_words.append(word) for word in para.split()]\n",
        "print(len(total_words))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "iQAQGF4vVy6y",
        "outputId": "f5ebe443-508a-46c5-91ed-b9e2f0f9e415"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "4523823\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import collections\n",
        "counter = collections.Counter(total_words)\n",
        "freq_cnt = dict(counter)"
      ],
      "metadata": {
        "id": "QOHLsaasV9yc"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "vocab = set()\n",
        "for ques in clean_questions:\n",
        "  [vocab.update(ques.split())]\n",
        "print(len(vocab))\n",
        "\n",
        "for para in clean_paragraphs:\n",
        "  [vocab.update(para.split())]\n",
        "print(len(vocab))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "LCoVb1PBzyJ3",
        "outputId": "aea94f3e-75ba-4932-e4a9-e03dfceb965e"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "31085\n",
            "83373\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "word_to_idx = {}\n",
        "idx_to_word = {}\n",
        "count = 0\n",
        "for i,word in enumerate(vocab):\n",
        "  if(freq_cnt[word]>3):\n",
        "    word_to_idx[word] = count\n",
        "    idx_to_word[count] = word\n",
        "    count+=1"
      ],
      "metadata": {
        "id": "4AeUAamP0rAm"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "len(word_to_idx)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "7J0D8OTcWn-K",
        "outputId": "0ed88dd6-a8c7-49ea-e115-411c526f715a"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "44306"
            ]
          },
          "metadata": {},
          "execution_count": 19
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "length = len(word_to_idx)"
      ],
      "metadata": {
        "id": "kcSNFG3uC5jC"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "length"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ejaDbU3GIc0C",
        "outputId": "e84eb17b-4e37-4077-badc-fc0c70418780"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "44306"
            ]
          },
          "metadata": {},
          "execution_count": 21
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "word_to_idx['startseq'] = length+1\n",
        "word_to_idx['endseq'] = length+2\n",
        "idx_to_word[length+1] = 'startseq'\n",
        "idx_to_word[length+2] = 'endseq'"
      ],
      "metadata": {
        "id": "6HPB5Aa-1AZb"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "len(word_to_idx)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "tZmKdozwG2NS",
        "outputId": "1abacae4-f131-4d96-9a29-99bd7826ca8a"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "44308"
            ]
          },
          "metadata": {},
          "execution_count": 23
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "len(clean_questions)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ZmIkF5eF1wUH",
        "outputId": "bda3d296-3dde-4fe8-bad5-4b594364df59"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "30463"
            ]
          },
          "metadata": {},
          "execution_count": 24
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Adding startseq and endseq to every questions\n",
        "for index,ques in enumerate(clean_questions):\n",
        "  clean_questions[index] = \"startseq \" + ques + \" endseq\""
      ],
      "metadata": {
        "id": "-BWs6kwd1RBa"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "clean_questions[0]"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 53
        },
        "id": "gLOlkXyc10_a",
        "outputId": "d6abaf88-3bf7-446b-8edc-f3f14ec06059"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'startseq malayali located language spoken else place known ever successful produce anything population still growing country thriving endseq'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 26
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "context_maxlen = 0\n",
        "for cont in clean_paragraphs:\n",
        "  context_maxlen = max(context_maxlen,len(cont.split()))\n",
        "question_maxlen = 0\n",
        "for ques in clean_questions:\n",
        "  question_maxlen = max(question_maxlen,len(ques.split()))"
      ],
      "metadata": {
        "id": "bawFHOBC4VdT"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "context_maxlen"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "9ItwQBqG4w2o",
        "outputId": "6727d13d-0864-481f-e4b1-aa108c70e874"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "942"
            ]
          },
          "metadata": {},
          "execution_count": 33
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "question_maxlen"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "rs-hCD404zGh",
        "outputId": "dd9b0ac0-32d1-429e-929a-b6ae5c82ca9d"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "128"
            ]
          },
          "metadata": {},
          "execution_count": 34
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import tensorflow as tf\n",
        "from tensorflow import keras\n",
        "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
        "from keras.layers import Dense,Dropout,Embedding,LSTM,Input\n",
        "from keras.layers.merging import add\n",
        "from keras.models import Model\n",
        "from keras.utils import to_categorical"
      ],
      "metadata": {
        "id": "cTtVmVfD2gTH"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# tpu = tf.distribute.cluster_resolver.TPUClusterResolver()\n",
        "# tf.config.experimental_connect_to_cluster(tpu)\n",
        "# tf.tpu.experimental.initialize_tpu_system(tpu)\n",
        "# tpu_strategy = tf.distribute.TPUStrategy(tpu)"
      ],
      "metadata": {
        "id": "h-PcB7FRoDjM"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "vocab_size = len(word_to_idx)+1"
      ],
      "metadata": {
        "id": "GG5Zi1fyu7Dq"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def datagenerator(vocab_size=vocab_size,context_data=clean_paragraphs,questions_data=clean_questions,maxlen_context=context_maxlen,maxlen_questions=question_maxlen,word_to_idx=word_to_idx,batch_size=14):\n",
        "  context = []\n",
        "  questions = []\n",
        "  generated = []\n",
        "  n=0\n",
        "\n",
        "  while(True):\n",
        "    for index in range(len(context_data)):\n",
        "      n+=1\n",
        "      para_sequence = [word_to_idx[word] for word in context_data[index].split() if word in word_to_idx.keys()]\n",
        "      xi2 = pad_sequences([para_sequence],maxlen=maxlen_context,value=0,padding='post')[0]\n",
        "      ques_sequence = [word_to_idx[word] for word in questions_data[index].split() if word in word_to_idx.keys()]\n",
        "      for i in range(1,len(ques_sequence)):\n",
        "        xi = ques_sequence[0:i]\n",
        "        yi = ques_sequence[i]\n",
        "        yi = to_categorical([yi-1],num_classes=vocab_size)[0]\n",
        "        xi = pad_sequences([xi],maxlen=maxlen_questions,value=0,padding='post')[0]\n",
        "\n",
        "        context.append(xi2)\n",
        "        questions.append(xi)\n",
        "        generated.append(yi)\n",
        "\n",
        "      if(n==batch_size):\n",
        "        yield [[tf.convert_to_tensor(context),tf.convert_to_tensor(questions)],tf.convert_to_tensor(generated)]\n",
        "        # yield [[np.array(context),np.array(questions)],np.array(generated)]\n",
        "        context,questions,generated = [],[],[]\n",
        "        n = 0"
      ],
      "metadata": {
        "id": "VLT5rwYAwuUW"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def get_embedding_matrix(vocab_size=len(word_to_idx)):\n",
        "  emb_dim = 50\n",
        "  matrix = np.zeros((vocab_size+1,emb_dim))\n",
        "  for word,idx in word_to_idx.items():\n",
        "    embedding_vector = embedding_index.get(word)\n",
        "    if embedding_vector is not None:\n",
        "      matrix[idx] = embedding_vector\n",
        "  return matrix"
      ],
      "metadata": {
        "id": "yYYO3TrVykqB"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "embedding_matrix = get_embedding_matrix()"
      ],
      "metadata": {
        "id": "gvoRm9zS3fw6"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "with tf.device('/device:GPU:0'):\n",
        "  embedding_matrix = tf.convert_to_tensor(embedding_matrix)"
      ],
      "metadata": {
        "id": "JzbIhKRCmIdf"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "embedding_matrix.shape"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "wGPWg1isX7V1",
        "outputId": "cf11df66-4ea0-4e6c-cfed-719a68d08a1c"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "TensorShape([44309, 50])"
            ]
          },
          "metadata": {},
          "execution_count": 34
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "input_context = Input(shape=(context_maxlen,))\n",
        "input_context1 = Embedding(input_dim=vocab_size,output_dim=50,mask_zero=True)(input_context)\n",
        "input_context2 = LSTM(128)(input_context1)\n",
        "# input_context3 = Dense(256,activation=\"relu\")(input_context2)\n",
        "\n",
        "input_questions = Input(shape=(question_maxlen,))\n",
        "input_questions1 = Embedding(input_dim=vocab_size,output_dim=50,mask_zero=True)(input_questions)\n",
        "input_questions2 = LSTM(128)(input_questions1)\n",
        "# input_questions3 = Dense(256,activation=\"relu\")(input_questions2)\n",
        "\n",
        "decoder = add([input_context2,input_questions2])\n",
        "decoder1 = Dense(512,activation=\"relu\")(decoder)\n",
        "outputs = Dense(vocab_size,activation=\"softmax\")(decoder1)\n",
        "\n",
        "model = Model(inputs=[input_context,input_questions],outputs=outputs)\n",
        "model.summary()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "QHCgmZe33nnw",
        "outputId": "232c5ecc-ba3f-4e31-f4bc-103d87287a44"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Model: \"model\"\n",
            "__________________________________________________________________________________________________\n",
            " Layer (type)                   Output Shape         Param #     Connected to                     \n",
            "==================================================================================================\n",
            " input_1 (InputLayer)           [(None, 942)]        0           []                               \n",
            "                                                                                                  \n",
            " input_2 (InputLayer)           [(None, 128)]        0           []                               \n",
            "                                                                                                  \n",
            " embedding (Embedding)          (None, 942, 50)      2215450     ['input_1[0][0]']                \n",
            "                                                                                                  \n",
            " embedding_1 (Embedding)        (None, 128, 50)      2215450     ['input_2[0][0]']                \n",
            "                                                                                                  \n",
            " lstm (LSTM)                    (None, 128)          91648       ['embedding[0][0]']              \n",
            "                                                                                                  \n",
            " lstm_1 (LSTM)                  (None, 128)          91648       ['embedding_1[0][0]']            \n",
            "                                                                                                  \n",
            " add (Add)                      (None, 128)          0           ['lstm[0][0]',                   \n",
            "                                                                  'lstm_1[0][0]']                 \n",
            "                                                                                                  \n",
            " dense (Dense)                  (None, 512)          66048       ['add[0][0]']                    \n",
            "                                                                                                  \n",
            " dense_1 (Dense)                (None, 44309)        22730517    ['dense[0][0]']                  \n",
            "                                                                                                  \n",
            "==================================================================================================\n",
            "Total params: 27,410,761\n",
            "Trainable params: 27,410,761\n",
            "Non-trainable params: 0\n",
            "__________________________________________________________________________________________________\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "model.layers[2].set_weights([embedding_matrix])\n",
        "model.layers[2].trainable = False\n",
        "model.layers[3].set_weights([embedding_matrix])\n",
        "model.layers[3].trainable = False\n",
        "\n",
        "with tf.device('/device:GPU:0'):\n",
        "  model.compile(loss=\"categorical_crossentropy\",optimizer=\"adam\")\n",
        "model.summary()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "j4hderYn7e0H",
        "outputId": "f8347d1b-96e1-498b-c006-f2de09622043"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Model: \"model\"\n",
            "__________________________________________________________________________________________________\n",
            " Layer (type)                   Output Shape         Param #     Connected to                     \n",
            "==================================================================================================\n",
            " input_1 (InputLayer)           [(None, 942)]        0           []                               \n",
            "                                                                                                  \n",
            " input_2 (InputLayer)           [(None, 128)]        0           []                               \n",
            "                                                                                                  \n",
            " embedding (Embedding)          (None, 942, 50)      2215450     ['input_1[0][0]']                \n",
            "                                                                                                  \n",
            " embedding_1 (Embedding)        (None, 128, 50)      2215450     ['input_2[0][0]']                \n",
            "                                                                                                  \n",
            " lstm (LSTM)                    (None, 128)          91648       ['embedding[0][0]']              \n",
            "                                                                                                  \n",
            " lstm_1 (LSTM)                  (None, 128)          91648       ['embedding_1[0][0]']            \n",
            "                                                                                                  \n",
            " add (Add)                      (None, 128)          0           ['lstm[0][0]',                   \n",
            "                                                                  'lstm_1[0][0]']                 \n",
            "                                                                                                  \n",
            " dense (Dense)                  (None, 512)          66048       ['add[0][0]']                    \n",
            "                                                                                                  \n",
            " dense_1 (Dense)                (None, 44309)        22730517    ['dense[0][0]']                  \n",
            "                                                                                                  \n",
            "==================================================================================================\n",
            "Total params: 27,410,761\n",
            "Trainable params: 22,979,861\n",
            "Non-trainable params: 4,430,900\n",
            "__________________________________________________________________________________________________\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "with tf.device('/device:GPU:0'):\n",
        "  epochs = 5\n",
        "  batch_size = 14\n",
        "  steps = len(clean_questions)//batch_size\n",
        "  for i in range(epochs):\n",
        "    generator = datagenerator(vocab_size,clean_paragraphs,clean_questions,context_maxlen,question_maxlen,word_to_idx,batch_size)\n",
        "    model.fit(generator,epochs=1,steps_per_epoch=steps,verbose=1,shuffle=True)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Nnf9yWVJzD5v",
        "outputId": "fde44c47-7bc6-4eb6-c525-b8df7e542361"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "2175/2175 [==============================] - 3458s 2s/step - loss: 7.7816\n",
            "  48/2175 [..............................] - ETA: 47:39 - loss: 6.2091"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# For TPU"
      ],
      "metadata": {
        "id": "8p8-AUWso-cU"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "with tpu_strategy.scope():\n",
        "  vocab_size = len(word_to_idx)+1\n",
        "  input_context = Input(shape=(context_maxlen,))\n",
        "  input_context1 = Embedding(input_dim=vocab_size,output_dim=50,mask_zero=True)(input_context)\n",
        "  input_context2 = LSTM(128)(input_context1)\n",
        "\n",
        "  input_questions = Input(shape=(question_maxlen,))\n",
        "  input_questions1 = Embedding(input_dim=vocab_size,output_dim=50,mask_zero=True)(input_questions)\n",
        "  input_questions2 = LSTM(128)(input_questions1)\n",
        "\n",
        "  decoder = add([input_context2,input_questions2])\n",
        "  decoder1 = Dense(512,activation=\"relu\")(decoder)\n",
        "  outputs = Dense(vocab_size,activation=\"softmax\")(decoder1)\n",
        "\n",
        "  model = Model(inputs=[input_context,input_questions],outputs=outputs)\n",
        "\n",
        "  model.layers[2].set_weights([embedding_matrix])\n",
        "  model.layers[2].trainable = False\n",
        "  model.layers[3].set_weights([embedding_matrix])\n",
        "  model.layers[3].trainable = False\n",
        "\n",
        "  model.compile(loss=\"categorical_crossentropy\",optimizer=\"adam\")\n",
        "model.summary()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "KIqe7uaFpAnZ",
        "outputId": "03c78d5a-1a9b-4a9c-ed23-85cf3464c683"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Model: \"model\"\n",
            "__________________________________________________________________________________________________\n",
            " Layer (type)                   Output Shape         Param #     Connected to                     \n",
            "==================================================================================================\n",
            " input_1 (InputLayer)           [(None, 942)]        0           []                               \n",
            "                                                                                                  \n",
            " input_2 (InputLayer)           [(None, 128)]        0           []                               \n",
            "                                                                                                  \n",
            " embedding (Embedding)          (None, 942, 50)      2215450     ['input_1[0][0]']                \n",
            "                                                                                                  \n",
            " embedding_1 (Embedding)        (None, 128, 50)      2215450     ['input_2[0][0]']                \n",
            "                                                                                                  \n",
            " lstm (LSTM)                    (None, 128)          91648       ['embedding[0][0]']              \n",
            "                                                                                                  \n",
            " lstm_1 (LSTM)                  (None, 128)          91648       ['embedding_1[0][0]']            \n",
            "                                                                                                  \n",
            " add (Add)                      (None, 128)          0           ['lstm[0][0]',                   \n",
            "                                                                  'lstm_1[0][0]']                 \n",
            "                                                                                                  \n",
            " dense (Dense)                  (None, 512)          66048       ['add[0][0]']                    \n",
            "                                                                                                  \n",
            " dense_1 (Dense)                (None, 44309)        22730517    ['dense[0][0]']                  \n",
            "                                                                                                  \n",
            "==================================================================================================\n",
            "Total params: 27,410,761\n",
            "Trainable params: 22,979,861\n",
            "Non-trainable params: 4,430,900\n",
            "__________________________________________________________________________________________________\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "epochs = 5\n",
        "batch_size = 14\n",
        "steps = len(clean_questions)//batch_size\n",
        "for i in range(epochs):\n",
        "  dataset = tf.data.Dataset.from_generator(datagenerator,output_types=(tf.dtypes.float32),output_shapes=(1))\n",
        "  print(dataset)\n",
        "  # generator = datagenerator(vocab_size,clean_paragraphs,clean_questions,context_maxlen,question_maxlen,word_to_idx,batch_size)\n",
        "  model.fit(dataset,epochs=1,steps_per_epoch=steps,verbose=1,shuffle=True)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 712
        },
        "id": "WGmvxD_wpeEx",
        "outputId": "f5aa3d55-801d-4e47-9066-e520b5a7b75b"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "<_FlatMapDataset element_spec=TensorSpec(shape=(1,), dtype=tf.float32, name=None)>\n"
          ]
        },
        {
          "output_type": "error",
          "ename": "ValueError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-50-7363aac1e671>\u001b[0m in \u001b[0;36m<cell line: 4>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      6\u001b[0m   \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdataset\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      7\u001b[0m   \u001b[0;31m# generator = datagenerator(vocab_size,clean_paragraphs,clean_questions,context_maxlen,question_maxlen,word_to_idx,batch_size)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 8\u001b[0;31m   \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdataset\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mepochs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0msteps_per_epoch\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0msteps\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mverbose\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mshuffle\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/keras/utils/traceback_utils.py\u001b[0m in \u001b[0;36merror_handler\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m     68\u001b[0m             \u001b[0;31m# To get the full stack trace, call:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     69\u001b[0m             \u001b[0;31m# `tf.debugging.disable_traceback_filtering()`\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 70\u001b[0;31m             \u001b[0;32mraise\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwith_traceback\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfiltered_tb\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     71\u001b[0m         \u001b[0;32mfinally\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     72\u001b[0m             \u001b[0;32mdel\u001b[0m \u001b[0mfiltered_tb\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/keras/engine/training.py\u001b[0m in \u001b[0;36mtf__train_function\u001b[0;34m(iterator)\u001b[0m\n\u001b[1;32m     13\u001b[0m                 \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     14\u001b[0m                     \u001b[0mdo_return\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mTrue\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 15\u001b[0;31m                     \u001b[0mretval_\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mag__\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mconverted_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mag__\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mld\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstep_function\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mag__\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mld\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mag__\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mld\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0miterator\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfscope\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     16\u001b[0m                 \u001b[0;32mexcept\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     17\u001b[0m                     \u001b[0mdo_return\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mFalse\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mValueError\u001b[0m: in user code:\n\n    File \"/usr/local/lib/python3.10/dist-packages/keras/engine/training.py\", line 1284, in train_function  *\n        return step_function(self, iterator)\n    File \"/usr/local/lib/python3.10/dist-packages/keras/engine/training.py\", line 1268, in step_function  **\n        outputs = model.distribute_strategy.run(run_step, args=(data,))\n    File \"/usr/local/lib/python3.10/dist-packages/keras/engine/training.py\", line 1249, in run_step\n        outputs = model.train_step(data)\n    File \"/usr/local/lib/python3.10/dist-packages/keras/engine/training.py\", line 1050, in train_step\n        y_pred = self(x, training=True)\n    File \"/usr/local/lib/python3.10/dist-packages/keras/utils/traceback_utils.py\", line 70, in error_handler\n        raise e.with_traceback(filtered_tb) from None\n    File \"/usr/local/lib/python3.10/dist-packages/keras/engine/input_spec.py\", line 219, in assert_input_compatibility\n        raise ValueError(\n\n    ValueError: Layer \"model\" expects 2 input(s), but it received 1 input tensors. Inputs received: [<tf.Tensor 'replicated_input_1:0' shape=(None,) dtype=float32>]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# for data in range(0,1):\n",
        "generator = datagenerator(vocab_size,clean_paragraphs,clean_questions,context_maxlen,question_maxlen,word_to_idx,10)\n",
        "print(model.predict(generator))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "XFuBl7Srm1Qm",
        "outputId": "24410650-cff4-4328-8c04-b8e75d2e56cd"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "    197/Unknown - 162s 826ms/step"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "epochs = 10\n",
        "batch_size = 14\n",
        "steps = len(clean_questions)//batch_size\n",
        "for i in range(epochs):\n",
        "  generator = datagenerator(vocab_size,clean_paragraphs,clean_questions,context_maxlen,question_maxlen,word_to_idx,batch_size)\n",
        "  model.fit(generator,epochs=1,steps_per_epoch=steps,verbose=1,shuffle=True)\n",
        "  break"
      ],
      "metadata": {
        "id": "yK_o1T7X8RZs",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 640
        },
        "outputId": "a389d1b1-79a4-4435-a21d-0ceebf43486e"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "tf.Tensor(\n",
            "[[33725  1583  4466 ...     0     0     0]\n",
            " [33725  1583  4466 ...     0     0     0]\n",
            " [33725  1583  4466 ...     0     0     0]\n",
            " ...\n",
            " [27094 36226 31283 ...     0     0     0]\n",
            " [27094 36226 31283 ...     0     0     0]\n",
            " [27094 36226 31283 ...     0     0     0]], shape=(287, 942), dtype=int32)\n",
            "tf.Tensor(\n",
            "[[27094 27815 11249 ...     0     0     0]\n",
            " [27094 27815 11249 ...     0     0     0]\n",
            " [27094 27815 11249 ...     0     0     0]\n",
            " ...\n",
            " [30542   624 15448 ...     0     0     0]\n",
            " [30542   624 15448 ...     0     0     0]\n",
            " [30542   624 15448 ...     0     0     0]], shape=(266, 942), dtype=int32)\n"
          ]
        },
        {
          "output_type": "error",
          "ename": "KeyboardInterrupt",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-43-578656d83747>\u001b[0m in \u001b[0;36m<cell line: 4>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mi\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mepochs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m   \u001b[0mgenerator\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdatagenerator\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mvocab_size\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mclean_paragraphs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mclean_questions\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mcontext_maxlen\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mquestion_maxlen\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mword_to_idx\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mbatch_size\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 6\u001b[0;31m   \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mgenerator\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mepochs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0msteps_per_epoch\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0msteps\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mverbose\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mshuffle\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      7\u001b[0m   \u001b[0;32mbreak\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/keras/utils/traceback_utils.py\u001b[0m in \u001b[0;36merror_handler\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m     63\u001b[0m         \u001b[0mfiltered_tb\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     64\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 65\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mfn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     66\u001b[0m         \u001b[0;32mexcept\u001b[0m \u001b[0mException\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     67\u001b[0m             \u001b[0mfiltered_tb\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_process_traceback_frames\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0me\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__traceback__\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/keras/engine/training.py\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self, x, y, batch_size, epochs, verbose, callbacks, validation_split, validation_data, shuffle, class_weight, sample_weight, initial_epoch, steps_per_epoch, validation_steps, validation_batch_size, validation_freq, max_queue_size, workers, use_multiprocessing)\u001b[0m\n\u001b[1;32m   1683\u001b[0m                         ):\n\u001b[1;32m   1684\u001b[0m                             \u001b[0mcallbacks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mon_train_batch_begin\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstep\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1685\u001b[0;31m                             \u001b[0mtmp_logs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrain_function\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0miterator\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1686\u001b[0m                             \u001b[0;32mif\u001b[0m \u001b[0mdata_handler\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshould_sync\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1687\u001b[0m                                 \u001b[0mcontext\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0masync_wait\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/tensorflow/python/util/traceback_utils.py\u001b[0m in \u001b[0;36merror_handler\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    148\u001b[0m     \u001b[0mfiltered_tb\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    149\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 150\u001b[0;31m       \u001b[0;32mreturn\u001b[0m \u001b[0mfn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    151\u001b[0m     \u001b[0;32mexcept\u001b[0m \u001b[0mException\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    152\u001b[0m       \u001b[0mfiltered_tb\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_process_traceback_frames\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0me\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__traceback__\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/tensorflow/python/eager/polymorphic_function/polymorphic_function.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *args, **kwds)\u001b[0m\n\u001b[1;32m    892\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    893\u001b[0m       \u001b[0;32mwith\u001b[0m \u001b[0mOptionalXlaContext\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_jit_compile\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 894\u001b[0;31m         \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    895\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    896\u001b[0m       \u001b[0mnew_tracing_count\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mexperimental_get_tracing_count\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/tensorflow/python/eager/polymorphic_function/polymorphic_function.py\u001b[0m in \u001b[0;36m_call\u001b[0;34m(self, *args, **kwds)\u001b[0m\n\u001b[1;32m    924\u001b[0m       \u001b[0;31m# In this case we have created variables on the first call, so we run the\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    925\u001b[0m       \u001b[0;31m# defunned version which is guaranteed to never create variables.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 926\u001b[0;31m       \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_no_variable_creation_fn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# pylint: disable=not-callable\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    927\u001b[0m     \u001b[0;32melif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_variable_creation_fn\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    928\u001b[0m       \u001b[0;31m# Release the lock early so that multiple threads can perform the call\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/tensorflow/python/eager/polymorphic_function/tracing_compiler.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m    141\u001b[0m       (concrete_function,\n\u001b[1;32m    142\u001b[0m        filtered_flat_args) = self._maybe_define_function(args, kwargs)\n\u001b[0;32m--> 143\u001b[0;31m     return concrete_function._call_flat(\n\u001b[0m\u001b[1;32m    144\u001b[0m         filtered_flat_args, captured_inputs=concrete_function.captured_inputs)  # pylint: disable=protected-access\n\u001b[1;32m    145\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/tensorflow/python/eager/polymorphic_function/monomorphic_function.py\u001b[0m in \u001b[0;36m_call_flat\u001b[0;34m(self, args, captured_inputs, cancellation_manager)\u001b[0m\n\u001b[1;32m   1755\u001b[0m         and executing_eagerly):\n\u001b[1;32m   1756\u001b[0m       \u001b[0;31m# No tape is watching; skip to running the function.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1757\u001b[0;31m       return self._build_call_outputs(self._inference_function.call(\n\u001b[0m\u001b[1;32m   1758\u001b[0m           ctx, args, cancellation_manager=cancellation_manager))\n\u001b[1;32m   1759\u001b[0m     forward_backward = self._select_forward_and_backward_functions(\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/tensorflow/python/eager/polymorphic_function/monomorphic_function.py\u001b[0m in \u001b[0;36mcall\u001b[0;34m(self, ctx, args, cancellation_manager)\u001b[0m\n\u001b[1;32m    379\u001b[0m       \u001b[0;32mwith\u001b[0m \u001b[0m_InterpolateFunctionError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    380\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mcancellation_manager\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 381\u001b[0;31m           outputs = execute.execute(\n\u001b[0m\u001b[1;32m    382\u001b[0m               \u001b[0mstr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msignature\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mname\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    383\u001b[0m               \u001b[0mnum_outputs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_num_outputs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/tensorflow/python/eager/execute.py\u001b[0m in \u001b[0;36mquick_execute\u001b[0;34m(op_name, num_outputs, inputs, attrs, ctx, name)\u001b[0m\n\u001b[1;32m     50\u001b[0m   \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     51\u001b[0m     \u001b[0mctx\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mensure_initialized\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 52\u001b[0;31m     tensors = pywrap_tfe.TFE_Py_Execute(ctx._handle, device_name, op_name,\n\u001b[0m\u001b[1;32m     53\u001b[0m                                         inputs, attrs, num_outputs)\n\u001b[1;32m     54\u001b[0m   \u001b[0;32mexcept\u001b[0m \u001b[0mcore\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_NotOkStatusException\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
          ]
        }
      ]
    }
  ]
}